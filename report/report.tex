\documentclass[a4paper, 14pt]{article}


\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{compat=1.9}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{bchart}




\begin{document}

	\renewcommand{\chaptername}{Лабораторная работа}
	\def\contentsname{Содержание}

	\begin{titlepage}
		\begin{center}
			\textsc{<<НАЦИОНАЛЬНЫЙ ИССЛЕДВАТЕЛЬСКИЙ УНИВЕРСИТЕТ ИТМО">\\[5mm]
			Факультет информационных технологий и программирования\\[2mm]
			Кафедра компьютерных технологий}

			\vfill

			\textbf{ОТЧЁТ ПО ЛАБОРАТОРНОЙ РАБОТЕ №4\\[3mm]
			Изучение алгоритмов метода Ньютона и его модификаций, в том числе квазиньютоновских методов.\\
			Вариант 2
\\[28mm]
			}
		\end{center}

		\hfill
		\begin{minipage}{.5\textwidth}
			Выполнили студенты:\\[2mm]
			Ефимов Сергей Алексеевич\\
			группа: М3237\\[2mm]
			Соколов Александр Андреевич\\
			группа: М3234\\[5mm]

			Проверил:\\[2mm]
			Свинцов Михаил Викторович
		\end{minipage}%
		\vfill
		\begin{center}
			г. Санкт-Петербург
		\end{center}
	\end{titlepage}


	\section*{Постановка задачи}
	Задача лаборатрной работы  -- Разработать программы для безусловной минимизации функций многих переменных
	
	Реализовать алгоритмы метода миимизации функции Ньютона:
	\begin{itemize}
		\item Классический
		\item С одномерным поиском
		\item C направлением спуска
	\end{itemize}
	Также необходимо провести исследование работы методов на различных функциях.
	В ходе работы необходимо разработать квазиньютоновский метод Бройдена-Флетчера-Шено и метод Пауэлла, проанализировать и сравнить с наилучшим методом Ньютона. 
	
	\section*{Ход работы}
		\subsubsection*{Теория}
		Рассмотрим задачу итерационной минимизации. Пусть $f(x) \in E^n$ - минимизируемая функция. Также $f(x)$ дважды дифференцируема, тогда, начав с точки $x_0$ мы можем построить квадратичную аппроксимацию $f(x)$ в окрестности $x_0$ на основе формулы Тейлора:
		\[
		f(x)  = f(x^{k-1}) + \left\langle \nabla f(x^{k-1}), x - x^{k-1}\right\rangle
		- \frac{1}{2} \left\langle H(x^{k})(x - x^{k-1}), x - x^{k-1} \right\rangle + 
		o(\vert x - x^{k-1} \vert)^2
		\]
		Пренебрегая остатком в форме Пеано получим:
		\[
		\phi_k(x)  = f(x^{k-1}) + \left\langle \nabla f(x^{k-1}), x - x^{k-1}\right\rangle
		- \frac{1}{2} \left\langle H(x^{k})(x - x^{k-1}), x - x^{k-1} \right\rangle 
		\]
		
		Если матрица Гессе является положительно определенной то $\phi_k$ имеет единственную точку минимума, которая и является следущей точкой итерационной последовательности. Данная точка может быть найдена из следущего условия:
		\[
		\nabla \phi_k(x) = \nabla f(k^{k-1}) + H(x^{k-1})(x - x^{k-1}) = 0		
		\]
		Исходя из формулы:
		\[
		\nabla(a^Tx) = a\]\[
		\nabla (x^TAx) = 2Ax		
		\]
		
		Тогда следущая точка релаксационной последовательности будет вычисляться как:
		\[
			x^k = x^{k-1} - H^{-1}(x^{k-1}) \nabla f(x^{k-1}); k \in N		
		\]
		Пусть $x'^k$ - вспомогательная точка релаксационной последовательности, тогда $x^k$ можно найти как:
		\[
			x^k = x^{k-1} +\alpha_k(x'^{k} - x^{k-1}) = x^{k-1} + \alpha_kp^k 	
		\]\[
		\alpha_k > 0\]
		где: $
		p^k= x'^k - x^{k - 1}$
		направление спуска 

	\subsubsection*{Классический метод Ньютона}
	
	Алгоритм:
	
	Пусть дано $x_0$ - начальное приближение, $\epsilon$ - точность, тогда:
	\begin{enumerate}
		\item Рассчитать $\nabla f(x)$ - Градиент от функции в текущем приближении, $H$ - матрицу Гессе по формуле $\nabla^2 f(x)$
		\item Решить СЛАУ $Hp^k = - \nabla f(x)$
		\item Вычислить следущий $x^k = x^{k-1} + p^k$
		\item Если $\Vert x^k - x^{k-1} \Vert < \epsilon$, что эквивалентно $\Vert p^k \Vert$, то текущее приближение является искомым решением, иначе повторить алгоритм с 1 пункта
		\begin{itemize}
		\item Минусы:
		\begin{itemize}
			\item Если начальное приближение выбрать достаточно далеко от минимума, то метод не сходится, такт как не обладает глобальной сходимостью
		\end{itemize}
		\item Плюсы:
		\begin{itemize}
			\item Если H удовлетворяет условию Липшица в окрестности решения поставленной задачи, то метод обладает квадратичной сходимостью.
		\end{itemize}
		
\end{itemize}		  
	\end{enumerate}
	
	
	\subsubsection*{Метод Ньютона с одномерным поиском}
	
	Пусть $x^{k-1}$ - одномерный поиск в направлении $p^k$:
	
	Тогда $\alpha_k = \min_\alpha(f(x^k + \alpha p^k)) $ - вычисляется для нового напрвления в вычислении текущего минимума.
	
	Алгоритм очень мохож на предыдущий, но теперь дополнительно вычисляется  $\alpha_k$
	
		 Алгоритм:
		 
		 \begin{enumerate}
		\item Рассчитать $\nabla f(x)$ - Градиент от функции в текущем приближении, $H$ - матрицу Гессе по формуле $\nabla^2 f(x)$
		\item Решить СЛАУ $Hp^k = - \nabla f(x)$
		\item $\alpha_k = \min_\alpha(f(x^k + \alpha p^k)) $
		\item Вычислить следущий $x^k = x^{k-1} + \alpha p^k$
		\item Если $\Vert x^k - x^{k-1} \Vert < \epsilon$, что эквивалентно $\Vert p^k \Vert$, то текущее приближение является искомым решением, иначе повторить алгоритм с 1 пункта
		\begin{itemize}
		\item Минусы:
		\begin{itemize}
			\item Эффективность алгоритма зависит от того, является ли $p_k$ - направлением спуска
		\end{itemize}
		\item Плюсы:
		\begin{itemize}
			\item Алгоритм обладает глобальной сходимостью в отличие от классического метода
		\end{itemize}
		
\end{itemize}		  
	\end{enumerate}
		 
		 	\subsubsection*{Метод Ньютона с направлением спуска}
	 Если  $p_k$ - направление спуска:
	 \[
	 (p^k)^T \nabla f(x^k) < 0\]
	 
	 Иначе $p^k$ - не напрвление спуска, тогда следует использовать 
	 $- \nabla(x^k)$, тогда:
	 \[
	 H(x^k)p^k = - \nabla f(x^k) \Rightarrow\]
	 
	 \begin{equation*}
p^k = 
 \begin{cases}
   p^k, &\text{$(p^k)^T \nabla f(x^k) < 0$}\\
   - \nabla f(x^k) &\text{ $(p^k)^T \nabla f(x^k) > 0 $}
 \end{cases}
\end{equation*}

Данный метод позволяет предотвратить неверное направление поиска, которы связан с седловыми точками, а так же точками максимума.
Остальные шаги аналогичны предыдущему методу
.Метод так же обладает глобальной сходимостью.
	\subsubsection*{Квазиньютоновские методы}

Квазиньютоновские методы - методы оптимизации, основанные на накоплении информации о кривизне целевой функции по наблюдениям за изменением градиента, чем принципиально отличаются от ньютоновских методов. Класс квазиньютоновских методов исключает явное формирование матрицы Гессе, заменяя её некоторым приближением. 

Квазиньютоновские методы:
\begin{itemize}
	\item Объединяют в себе достоиства от наискорейшего спуска и метода Ньютона
	\item Не требуют обращение к матрице H
	\item Сохраняют высокую сходимость итерационной последовательности 
\end{itemize}

Общий вид релаксационной последовательности
\[x^k = x^{k - 1} + \alpha_k p^k\]
где $p^k$ - направление спуска
\[p^k = G_k w^k,  k \in N\]
\[
w^k = - \nabla f(x^{k- 1})\]
$G_k$ - Положительно определенная матрица (n x n) специального вида

Вичисление матрицы $G_k$ происходит следущим образом, Она должна сходится к обратной матрица Гессе при достаточно больших k, то есть:

\[G_{k \longrightarrow \infty} \longrightarrow H^{-1}(x^*)\]

где $x^*$ - точка минимума

За счет этого метод может гарантировать высокую сходимость, присущую метода Ньютона

$\alpha_k$ выбирается одним из типовых способов:

\begin{enumerate}
\item Константное значение $\alpha_k = 1$
\item Дробление шага
\item Частоиспользуемый вариант выбора $\alpha_k$ использование исчерпывающего спуска  напралении $p^k$
\end{enumerate} 

Идеей квазиньютоновских методов является удачный выбор апроксимации, который может занчительно сократить объем вычислений по сравнению c обращением H, тем самым упростить процедуру построения $p^k$

Методы обладают глобальной сходимостью.

\subsubsection*{Метод Бройдена-Флетчера-Шено}
Метод Бройдена-Флетчера-Шено - один из наиболее широко применяемых квазиньютоновских методов

Свойства присущие данному методу:

\begin{itemize}
	\item  $G_k$ - сохраняет положительную определенность
	\item Если $G_k$ - симметричная, то $G_{k+1}$ - тоже симметричная
	\item При минимизации квадаратичной функции с положительно опреленной матрицей А метод сводится к методу сопряженных направлений, точное решение не более чем за n итераций
	\item Матрицы $G_k$ связанны равенством
	\[
	G_k \cdot A \cdot p_i = p_i\]
	Следовательно $G_k$ - обратная матрица к Гессиану
	\item Если целевая функция не квадратичная, то метод не позволяет найти решение за конечое кол-во итераций. Для уменьшения ошибки принято первые n итераций $G_k = I$(единичная матрица)
	\item Если целевая функция  квадаратичная то
	\[
	H^{-1} = \sum_{i = 1} n\frac{\triangle x_i(\triangle x_i)^T}{i(\triangle x_i)^T \triangle w_i} \]
\end{itemize}

На первой итерации $G1 = I$
$w_1 =  - \nabla f(x_0)$

$p_1 = w_1$

$\alpha_1 = \min_\alpha f(x_0 \alpha p_1)$

$x_1 = x_0 + \alpha_1 p_1$

$\triangle x_1 = x_1 - x_0$

 Если k > 0, то
$w_k =  - \nabla f(x_{k - 1})$

$\triangle  w_k = w_k - w_{k-1}$

$p_k = G_k \cdot w_k$

$\alpha_k = \min_\alpha f(x_{k - 1} \alpha_k p_k)$

$x_k = x_{k-1} + \alpha_k p_k$

$\triangle x_k = x_k - x_{k - 1}$

Условие остановы: $\Vert \triangle x_k \Vert < \epsilon$

\[G_{k+1} = G_k - \frac{\triangle x_i(\triangle x_i)^T}{\langle \triangle w^k, \triangle x^k\rangle} - \frac{G_k \triangle w^k (\triangle w^k)^T G_k^T}{\rho_k} + \rho_k r^k (r^k)^T\]
\[
r^k = \frac{G_k \triangle w^k}{\rho_k} - \frac{\triangle x^k}{\langle \triangle x^k, \triangle w^k \rangle}\]
\[
\rho_k = \langle G_k \triangle w^k, \triangle w^k \rangle\]

\subsubsection*{Метод Пауэлла}
Очень сильно похож на предыдущий метод, за исключением определения $G_k$:

\[G_{k + 1} = G_k - \frac{\triangle x'_k (\triangle x'_k)^T}{\langle \triangle w_k, \triangle x'_k \rangle} \] 


\subsubsection*{Демострация методов на различных функциях}
	Проведем исследование на двух функциях:
	\begin{enumerate}
	\item 	 $\sin(x)$ 
	\item  $10x^2 + 2xy + 12y^2$ 
\end{enumerate}
\begin{itemize}
\item 
	\begin{itemize}
		\item Классический метод Ньютона: $\sin(x)$\\
		Начальное приближение: $x_0 = 1$\\
		\begin{tabular}{  |c | c | c| c| c|}
\hline
Кол-во итераций & $x_k$ & $\alpha_k$ & $p_k$  & $f(x_k)$\\ \hline
0 & 1.6721 & 1 & 0.5623 & 0.9812215 \\
1 & 1.5707  & 1& -0.6782 & 1\\
2 & 1.5707 & 1 & 0.0015 & 1 \\
3 & 1.5707 & 1 & 0 & 1 \\
4 & 1.5707 & 1 & 0 & 1 \\
\hline
\end{tabular}

Очевидно, что 1 не мининиум функции $\sin(x)$. Это наглядно показывает что классический метод Ньютона не обладает глобальной сходимостью.
		
	\end{itemize}
\end{itemize}


 





\end{document}

